{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Natural language processing in TensorFlow\n",
    "Text is messier than images (short and long sentences). How to represent words and sentences as numbers for learning?\n",
    "Week 1: load text, preprocess it and setup the data so it can be fed to NN.\n",
    "We start by building models to represent text (sentiment in text). It will be trained on a labeled text and then classify a new text based on what they've seen.\n",
    "Pixel values were already numbers, but what happens with text? How can we do a sentiment analysis with words?\n",
    "\n",
    "## Word based encodings\n",
    "We could take character encodings for each character of a set (e.g. ASCII), but would that help us to understand the meaning of the word?\n",
    "```\n",
    "`LISTEN` and `SILENT` - would have the same representation but they mean different things!\n",
    "```\n",
    "\n",
    "What if we give value to each word?\n",
    "```\n",
    "I Love my dog\n",
    "1 2     3  4\n",
    "\n",
    "I Love my cat\n",
    "1 2     3  5\n",
    "\n",
    "Now we can see some similarities in sentences\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Using APIs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # one of the ways to encode words and transform sentences into vectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!', # the '!' will be stripped off by tokenizer\n",
    "    'Do you think my dog is amazing?' # a longer sentence to demonstrate padding\n",
    "]\n",
    "\n",
    "# num_words - a total amount of unique distinct words; Tokenizer will take top 100 words by value and encode those\n",
    "# Worth experimenting with. sometimes the impact of less words can be minimal in accuracy but huge in training time. Use with care!\n",
    "# Tokenizer strips punctuation out and lowercases the sentence\n",
    "# Out Of Vocabulary - a special token for unknown words, it must be something unique that does not appear in text.\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences) # takes the data and encodes it\n",
    "word_index = tokenizer.word_index # returns a dictionary of `word->word_token`\n",
    "print(word_index)\n",
    "\n",
    "# Text to sequence: turn sentences into a list of numbers based on the tokens.\n",
    "sequences = tokenizer.texts_to_sequences(sentences) # will convert sentences to a set of sequences\n",
    "print(sequences)\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "# to put padding zeros in the end and override max sentence length; this might lead to loss of data (truncating) that can also be overridden:\n",
    "# padded = pad_sequences(sequences, padding='post', truncating='post, maxlen=5)\n",
    "print(padded)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking more at the Tokenizer\n",
    "Lessons learned:\n",
    "- we need a lot of training data to get a broad vocabulary to make sure we do not get many unknown words\n",
    "- In many cases it is a good idea to set a special value for unknown words instead of just ignoring them using `oov_token`\n",
    "\n",
    "## Padding\n",
    "Padding is a technique that provides a uniformity of text size."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sarcasm dataset\n",
    "[High quality dataset for the task of Sarcasm Detection](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"sarcasm.json\", \"r\") as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])\n",
    "\n",
    "# Working with tokenizer:\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padded='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}