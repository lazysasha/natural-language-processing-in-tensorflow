{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 3: Sequence models (RNN, LSTM)\n",
    "Tokenizing words gave us a negative effect last week because the context of words was hard to follow when the words were broken down into subwords.\n",
    "The sequence in which the tokens of subwords appear becomes very important when understanding their meaning.\n",
    "\n",
    "In RNN, apart from x and y, there is always an element that is fed into a function from a previous function.\n",
    "```\n",
    "y0    y1    y2\n",
    "^     ^     ^\n",
    "| f0  | f1  | f2\n",
    "F --> F --> F -->\n",
    "^     ^     ^\n",
    "|     |     |\n",
    "x0    x1    x2\n",
    "```\n",
    "\n",
    "There can be a limitation when approaching text classification in this way:\n",
    "`Today has a beautiful blue <...>` -> \"blue sky\" - the context word that lets us understand the next word is very close to it.\n",
    "\n",
    "`I lived in Ireland, so at school they made me learn how to speak <...>` -> \"to speak Galeic\" - word \"Ireland\" is a context word but it apeears very far from the predicting word.\n",
    "\n",
    "In this case using LSTM (Long Short-Term Memory) network can be useful to use. LSTM has an additional pipeline of context called \"Cell state\" and this can be passed through the network.\n",
    "This helps to keep context from earlier tokens relevant to later ones.\n",
    "Cell State can also be bi-directional: later contexts can impact earlier ones."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Implementing LSTMs in code\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # LSTM layer here! 64 - is the # of outputs for this layer\n",
    "    # `Bidirectional` layer will make sure cells will go both ways, hence output shape will be 2 * 64 = 128\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # LSTMs can also be stacked\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Using a Convolutional network\n",
    "anotherModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # Use convolutions\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'), # 128 filters each for 5 words\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Going back to the IMDB dataset\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # tf.keras.layers.Flatten(), # would produce 171,533 params with high acc but clear overfitting\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # with LSTM we have only 30,129 params, but it will take ~43 sec per epoch\n",
    "\n",
    "    #tf.keras.layers.Conv1D(128, 5, activation='relu'), # with convolution layer it would be 171,149 params and it only takes about 6s/epoch\n",
    "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)), # would have 169,997 params, training time would take ~20s per epoch and accuracy will good, but still overfit\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Embedding(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}