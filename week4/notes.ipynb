{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 4: Sequence models and literature (Text Generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away ... ...\"\n",
    "corpus = data.lower().split('\\n') # split to create a list of sentences\n",
    "\n",
    "tokenizer.fit_on_texts(corpus) # Create a dictionary (key-value pairs, where key is the word and value is a token for that word) of words and the overall corpus\n",
    "total_words = len(tokenizer.word_index) + 1 # find total number of words in a corpus\n",
    "\n",
    "# Training the data\n",
    "input_sentences = [] # Training x's\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] # this will convert a line of text into a a list of tokens representing the words (e.g.: [4 2 66 8 67])\n",
    "    for i in range(1, len(token_list)): # iterate over a list of tokens and create a number of n-gram sequences:\n",
    "        # [4 2 66 8 67] ->\n",
    "        # [4 2]\n",
    "        # [4 2 66]\n",
    "        # [4 2 66 8]\n",
    "        # [4 2 66 8 67]\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sentences.append(n_gram_sequence)\n",
    "\n",
    "# Find the max length of the corpus:\n",
    "max_sequence_len = max([len(x) for x in input_sentences])\n",
    "\n",
    "# Pad all of the sequences so they are of the same length (pre-pad with zeros to make it easier to extract the label):\n",
    "input_sentences = np.array(pad_sequences(input_sentences, maxlen=max_sequence_len, padding='pre'))\n",
    "# So now we get this:\n",
    "# [0 0 0 4 2]\n",
    "# [0 0 4 2 66]\n",
    "# [0 4 2 66 8]\n",
    "# [4 2 66 8 67]\n",
    "\n",
    "# Now turn sequences into X's and Y's (input values and the labels)\n",
    "# General idea: use the last character as our label (Y) and take all but the last character as the input (X):\n",
    "# [0 0 0 4 2] -> X: [0 0 0 4] Y: 2\n",
    "# [0 0 4 2 66] -> X: [0 0 0 4 2] Y: 66\n",
    "# [0 4 2 66 8] -> X: [0 0 0 4 2 66] Y: 8\n",
    "# [4 2 66 8 67] -> X: [0 0 0 4 2 66 8] Y: 67\n",
    "xs = input_sentences[:,:-1] # take first N tokens (sliced to remove the last token)\n",
    "labels = input_sentences[:, -1] # take the last token (all sliced to keep the last token)\n",
    "\n",
    "# Apply One-Hot encoding to the labels (as this is a classification problem: given a sequence of words, classify from the corpus what the next word would likely be):\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) # every word gets its own class\n",
    "\n",
    "# Finding what the next word should be\n",
    "# Create a NN to classify that the next word should be given a set of words\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len - 1)) # handle all of our words, use 64 dimensions to set a vector for a word (can be tweaked), the size of the input dimensions is a length of the longest sequence minus 1d;\n",
    "# we subtract 1 because we cropped off the last word of each sequence to get a label (Y)!\n",
    "\n",
    "#model.add(LSTM(20)) # Carry context along for 2 units (can be tweaked)\n",
    "# As a result, the model will give lots of repetitions of words, this is because our LSTM was carrying a context forward.\n",
    "# We can change it to be bidirectional:\n",
    "model.add(Bidirectional(LSTM(20))) # Carry context along for 20 units (can be tweaked) and do it in both directions; it will converge much quicker and will produce less repetitions\n",
    "model.add(Dense(total_words, activation='softmax')) # Same size as total amount of words (same as used for One-Hot encoding); This layer will have 1 neuron per word.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # We do categorical classification.\n",
    "model.fit(xs, ys, epochs=500, verbose=1) # train for a lot of epochs since it takes a while for a model like this to converge, because it has very little data\n",
    "\n",
    "# To start predicting a word, we need a sentence to start from aka 'seed'.\n",
    "seed_text = 'Laurence went to dublin'\n",
    "next_words = 10 # predict next 10 words, the more words we predict the more uncertainty there is in every sentence; using a larger corpus would help\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer. texts_to_sequences([seed_text])[0] # tokenize the seed text, OOV words (e.g. Laurence) will be ignored: [134 13 49]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre') # pad the sequence to match it with the one in a training set: [0 0 0 0 134 13 59]\n",
    "    predicted = model.predict_classes(token_list, verbose=0) # pass it to the model and get a prediction back - this will give a token of the word most likely to be the next one in the sequence\n",
    "    output_word = \"\"\n",
    "\n",
    "    # perform a reverse lookup to turn token back into a word:\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Poetry!\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt \\\n",
    "    -O /tmp/irish-lyrics-eof.txt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "data = open('/tmp/irish-lyrics-eof.txt'). read()\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) # where 100 is a dimensionality of the embedding\n",
    "model.add(Bidirectional(LSTM(150))) # LSTM units is set to 150\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=100, verbose=1) # the more epochs is generally better, but eventually we hit a law of deminishing returns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}