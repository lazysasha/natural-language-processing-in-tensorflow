{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 4: Sequence models and literature (Text Generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away ... ...\"\n",
    "corpus = data.lower().split('\\n') # split to create a list of sentences\n",
    "\n",
    "tokenizer.fit_on_texts(corpus) # Create a dictionary (key-value pairs, where key is the word and value is a token for that word) of words and the overall corpus\n",
    "total_words = len(tokenizer.word_index) + 1 # find total number of words in a corpus\n",
    "\n",
    "# Training the data\n",
    "input_sentences = [] # Training x's\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] # this will convert a line of text into a a list of tokens representing the words (e.g.: [4 2 66 8 67])\n",
    "    for i in range(1, len(token_list)): # iterate over a list of tokens and create a number of n-gram sequences:\n",
    "        # [4 2 66 8 67] ->\n",
    "        # [4 2]\n",
    "        # [4 2 66]\n",
    "        # [4 2 66 8]\n",
    "        # [4 2 66 8 67]\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sentences.append(n_gram_sequence)\n",
    "\n",
    "# Find the max length of the corpus:\n",
    "max_sequence_len = max([len(x) for x in input_sentences])\n",
    "\n",
    "# Pad all of the sequences so they are of the same length (pre-pad with zeros to make it easier to extract the label):\n",
    "input_sentences = np.array(pad_sequences(input_sentences, maxlen=max_sequence_len, padding='pre'))\n",
    "# So now we get this:\n",
    "# [0 0 0 4 2]\n",
    "# [0 0 4 2 66]\n",
    "# [0 4 2 66 8]\n",
    "# [4 2 66 8 67]\n",
    "\n",
    "# Now turn sequences into X's and Y's (input values and the labels)\n",
    "# General idea: use the last character as our label (Y) and take all but the last character as the input (X):\n",
    "# [0 0 0 4 2] -> X: [0 0 0 4] Y: 2\n",
    "# [0 0 4 2 66] -> X: [0 0 0 4 2] Y: 66\n",
    "# [0 4 2 66 8] -> X: [0 0 0 4 2 66] Y: 8\n",
    "# [4 2 66 8 67] -> X: [0 0 0 4 2 66 8] Y: 67\n",
    "xs = input_sentences[:,:-1] # take first N tokens (sliced to remove the last token)\n",
    "labels = input_sentences[:, -1] # take the last token (all sliced to keep the last token)\n",
    "\n",
    "# Apply One-Hot encoding to the labels (as this is a classification problem: given a sequence of words, classify from the corpus what the next word would likely be):\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) # every word gets its own class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}